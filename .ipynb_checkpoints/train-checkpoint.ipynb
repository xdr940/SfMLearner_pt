{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import custom_transforms\n",
    "import models\n",
    "from utils import tensor2array, save_checkpoint, save_path_formatter, log_output_tensorboard\n",
    "from inverse_warp import inverse_warp\n",
    "\n",
    "from loss_functions import photometric_reconstruction_loss, explainability_loss, smooth_loss, compute_errors\n",
    "from logger import TermLogger, AverageMeter\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Structure from Motion Learner training on KITTI and CityScapes Dataset',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "\n",
    "parser.add_argument(\"--data\",help='path to original dataset',default='processed_data/')#处理完的训练集要包含sequence 和两个txt文档\n",
    "\n",
    "parser.add_argument('--dataset-format', default='sequential', metavar='STR',\n",
    "                    help='dataset format, stacked: stacked frames (from original TensorFlow code) \\\n",
    "                    sequential: sequential folders (easier to convert to with a non KITTI/Cityscape dataset')#训练集形式\n",
    "parser.add_argument('--sequence-length', type=int, metavar='N', help='sequence length for training', default=3)#？\n",
    "parser.add_argument('--rotation-mode', type=str, choices=['euler', 'quat'], default='euler',\n",
    "                    help='rotation mode for PoseExpnet : euler (yaw,pitch,roll) or quaternion (last 3 coefficients)')\n",
    "parser.add_argument('--padding-mode', type=str, choices=['zeros', 'border'], default='zeros',\n",
    "                    help='padding mode for image warping : this is important for photometric differenciation when going outside target image.'\n",
    "                         ' zeros will null gradients outside target image.'\n",
    "                         ' border will only null gradients of the coordinate outside (x or y)')\n",
    "parser.add_argument('--with-gt', action='store_true', help='use ground truth for validation. \\\n",
    "                    You need to store it in npy 2D arrays see data/kitti_raw_loader.py for an example')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers')\n",
    "parser.add_argument('--epochs', default=200, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--epoch-size', default=10, type=int, metavar='N',\n",
    "                    help='manual epoch size (will match dataset size if not set)')\n",
    "parser.add_argument('-b', '--batch-size', default=6, type=int,\n",
    "                    metavar='N', help='mini-batch size')\n",
    "parser.add_argument('--lr', '--learning-rate', default=2e-4, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum for sgd, alpha parameter for adam')\n",
    "parser.add_argument('--beta', default=0.999, type=float, metavar='M',\n",
    "                    help='beta parameters for adam')\n",
    "parser.add_argument('--weight-decay', '--wd', default=0, type=float,\n",
    "                    metavar='W', help='weight decay')\n",
    "parser.add_argument('--print-freq', default=10, type=int,\n",
    "                    metavar='N', help='print frequency')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set')\n",
    "parser.add_argument('--pretrained-disp', dest='pretrained_disp', default=None, metavar='PATH',\n",
    "                    help='path to pre-trained dispnet model')\n",
    "parser.add_argument('--pretrained-exppose', dest='pretrained_exp_pose', default=None, metavar='PATH',\n",
    "                    help='path to pre-trained Exp Pose net model')\n",
    "parser.add_argument('--seed', default=0, type=int, help='seed for random functions, and network initialization')\n",
    "parser.add_argument('--log-summary', default='progress_log_summary.csv', metavar='PATH',\n",
    "                    help='csv where to save per-epoch train and valid stats')\n",
    "parser.add_argument('--log-full', default='progress_log_full.csv', metavar='PATH',\n",
    "                    help='csv where to save per-gradient descent train stats')\n",
    "parser.add_argument('-p', '--photo-loss-weight', type=float, help='weight for photometric loss', metavar='W', default=1)\n",
    "parser.add_argument('-m', '--mask-loss-weight', type=float, help='weight for explainabilty mask loss', metavar='W', default=0)\n",
    "parser.add_argument('-s', '--smooth-loss-weight', type=float, help='weight for disparity smoothness loss', metavar='W', default=0.1)\n",
    "parser.add_argument('--log-output', action='store_true', help='will log dispnet outputs and warped imgs at validation step')\n",
    "parser.add_argument('-f', '--training-output-freq', type=int, help='frequence for outputting dispnet outputs and warped imgs at training for all scales if 0 will not output',\n",
    "                    metavar='N', default=0)\n",
    "\n",
    "best_error = -1\n",
    "n_iter = 0\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global best_error, n_iter, device\n",
    "    args = parser.parse_args()\n",
    "    if args.dataset_format == 'stacked':\n",
    "        from datasets.stacked_sequence_folders import SequenceFolder\n",
    "    elif args.dataset_format == 'sequential':\n",
    "        from datasets.sequence_folders import SequenceFolder\n",
    "    save_path = save_path_formatter(args, parser)\n",
    "    args.save_path = 'checkpoints'/save_path\n",
    "    print('=> will save everything to {}'.format(args.save_path))\n",
    "    args.save_path.makedirs_p()\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.evaluate:\n",
    "        args.epochs = 0\n",
    "\n",
    "    training_writer = SummaryWriter(args.save_path)\n",
    "    output_writers = []\n",
    "    if args.log_output:\n",
    "        for i in range(3):\n",
    "            output_writers.append(SummaryWriter(args.save_path/'valid'/str(i)))\n",
    "\n",
    "    # Data loading code\n",
    "    normalize = custom_transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                            std=[0.5, 0.5, 0.5])\n",
    "    train_transform = custom_transforms.Compose([\n",
    "        custom_transforms.RandomHorizontalFlip(),\n",
    "        custom_transforms.RandomScaleCrop(),\n",
    "        custom_transforms.ArrayToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    valid_transform = custom_transforms.Compose([custom_transforms.ArrayToTensor(), normalize])\n",
    "\n",
    "    print(\"=> fetching scenes in '{}'\".format(args.data))\n",
    "    train_set = SequenceFolder(\n",
    "        args.data,\n",
    "        transform=train_transform,\n",
    "        seed=args.seed,\n",
    "        train=True,\n",
    "        sequence_length=args.sequence_length\n",
    "    )\n",
    "\n",
    "    # if no Groundtruth is avalaible, Validation set is the same type as training set to measure photometric loss from warping\n",
    "    if args.with_gt:\n",
    "        from datasets.validation_folders import ValidationSet\n",
    "        val_set = ValidationSet(\n",
    "            args.data,\n",
    "            transform=valid_transform\n",
    "        )\n",
    "    else:\n",
    "        val_set = SequenceFolder(\n",
    "            args.data,\n",
    "            transform=valid_transform,\n",
    "            seed=args.seed,\n",
    "            train=False,\n",
    "            sequence_length=args.sequence_length,\n",
    "        )\n",
    "    print('{} samples found in {} train scenes'.format(len(train_set), len(train_set.scenes)))#训练集都是序列,不用左右\n",
    "    print('{} samples found in {} valid scenes'.format(len(val_set), len(val_set.scenes)))#测试集也是序列,不需要左右\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,#打乱\n",
    "        num_workers=args.workers,#多线程读取数据\n",
    "        pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataset=val_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,#不打乱\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True)\n",
    "\n",
    "    if args.epoch_size == 0:\n",
    "        args.epoch_size = len(train_loader)\n",
    "\n",
    "    # create model\n",
    "    print(\"=> creating model\")\n",
    "\n",
    "    disp_net = models.DispNetS().to(device)\n",
    "    output_exp = args.mask_loss_weight > 0\n",
    "    if not output_exp:\n",
    "        print(\"=> no mask loss, PoseExpnet will only output pose\")\n",
    "    pose_exp_net = models.PoseExpNet(nb_ref_imgs=args.sequence_length - 1, output_exp=args.mask_loss_weight > 0).to(device)\n",
    "\n",
    "    if args.pretrained_exp_pose:\n",
    "        print(\"=> using pre-trained weights for explainabilty and pose net\")\n",
    "        weights = torch.load(args.pretrained_exp_pose)\n",
    "        pose_exp_net.load_state_dict(weights['state_dict'], strict=False)\n",
    "    else:\n",
    "        pose_exp_net.init_weights()\n",
    "\n",
    "    if args.pretrained_disp:\n",
    "        print(\"=> using pre-trained weights for Dispnet\")\n",
    "        weights = torch.load(args.pretrained_disp)\n",
    "        disp_net.load_state_dict(weights['state_dict'])\n",
    "    else:\n",
    "        disp_net.init_weights()\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    disp_net = torch.nn.DataParallel(disp_net)\n",
    "    pose_exp_net = torch.nn.DataParallel(pose_exp_net)\n",
    "\n",
    "    print('=> setting adam solver')\n",
    "\n",
    "    optim_params = [\n",
    "        {'params': disp_net.parameters(), 'lr': args.lr},\n",
    "        {'params': pose_exp_net.parameters(), 'lr': args.lr}\n",
    "    ]\n",
    "    optimizer = torch.optim.Adam(optim_params,\n",
    "                                 betas=(args.momentum, args.beta),\n",
    "                                 weight_decay=args.weight_decay)\n",
    "\n",
    "    with open(args.save_path/args.log_summary, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter='\\t')\n",
    "        writer.writerow(['train_loss', 'validation_loss'])\n",
    "\n",
    "    with open(args.save_path/args.log_full, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter='\\t')\n",
    "        writer.writerow(['train_loss', 'photo_loss', 'explainability_loss', 'smooth_loss'])\n",
    "    n_epochs=args.epochs\n",
    "    train_size = min(len(train_loader), args.epoch_size)\n",
    "    valid_size = len(val_loader)\n",
    "    #print(n_epochs,train_size,valid_size)\n",
    "    #print(type(n_epochs),type(train_size),type(valid_size))\n",
    "\n",
    "    logger = TermLogger(n_epochs=args.epochs, train_size=min(len(train_loader), args.epoch_size), valid_size=len(val_loader))\n",
    "    logger.epoch_bar.start()\n",
    "\n",
    "    if args.pretrained_disp or args.evaluate:\n",
    "        logger.reset_valid_bar()\n",
    "        if args.with_gt:\n",
    "            errors, error_names = validate_with_gt(args, val_loader, disp_net, 0, logger, output_writers)\n",
    "        else:\n",
    "            errors, error_names = validate_without_gt(args, val_loader, disp_net, pose_exp_net, 0, logger, output_writers)\n",
    "        for error, name in zip(errors, error_names):\n",
    "            training_writer.add_scalar(name, error, 0)\n",
    "        error_string = ', '.join('{} : {:.3f}'.format(name, error) for name, error in zip(error_names[2:9], errors[2:9]))\n",
    "        logger.valid_writer.write(' * Avg {}'.format(error_string))\n",
    "    #main cycle\n",
    "    for epoch in range(args.epochs):\n",
    "        logger.epoch_bar.update(epoch)\n",
    "\n",
    "        ''' train for one epoch'''\n",
    "        logger.reset_train_bar()\n",
    "        train_loss = train(args, train_loader, disp_net, pose_exp_net, optimizer, args.epoch_size, logger, training_writer)\n",
    "        logger.train_writer.write(' * Avg Loss : {:.3f}'.format(train_loss))\n",
    "\n",
    "        # evaluate on validation set\n",
    "        logger.reset_valid_bar()\n",
    "        if args.with_gt:\n",
    "            errors, error_names = validate_with_gt(args, val_loader, disp_net, epoch, logger, output_writers)\n",
    "        else:\n",
    "            errors, error_names = validate_without_gt(args, val_loader, disp_net, pose_exp_net, epoch, logger, output_writers)\n",
    "\n",
    "        error_string = ', '.join('{} : {:.3f}'.format(name, error) for name, error in zip(error_names, errors))\n",
    "        logger.valid_writer.write(' * Avg {}'.format(error_string))\n",
    "\n",
    "        for error, name in zip(errors, error_names):\n",
    "            training_writer.add_scalar(name, error, epoch)\n",
    "\n",
    "        # Up to you to chose the most relevant error to measure your model's performance, careful some measures are to maximize (such as a1,a2,a3)\n",
    "        decisive_error = errors[1]\n",
    "        if best_error < 0:\n",
    "            best_error = decisive_error\n",
    "\n",
    "        # remember lowest error and save checkpoint\n",
    "        is_best = decisive_error < best_error\n",
    "        best_error = min(best_error, decisive_error)\n",
    "        save_checkpoint(\n",
    "            args.save_path, {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': disp_net.module.state_dict()\n",
    "            }, {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': pose_exp_net.module.state_dict()\n",
    "            },\n",
    "            is_best)\n",
    "\n",
    "        with open(args.save_path/args.log_summary, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter='\\t')\n",
    "            writer.writerow([train_loss, decisive_error])\n",
    "    logger.epoch_bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_loader, disp_net, pose_exp_net, optimizer, epoch_size, logger, train_writer):\n",
    "    global n_iter, device\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter(precision=4)\n",
    "    w1, w2, w3 = args.photo_loss_weight, args.mask_loss_weight, args.smooth_loss_weight\n",
    "\n",
    "    # switch to train mode\n",
    "    disp_net.train()\n",
    "    pose_exp_net.train()\n",
    "\n",
    "    end = time.time()\n",
    "    logger.train_bar.update(0)\n",
    "    for i, (tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(train_loader):\n",
    "    #for (i, data) in enumerate(train_loader):\n",
    "        log_losses = i > 0 and n_iter % args.print_freq == 0\n",
    "        log_output = args.training_output_freq > 0 and n_iter % args.training_output_freq == 0\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        tgt_img = tgt_img.to(device)#(4,3,128,416)\n",
    "        ref_imgs = [img.to(device) for img in ref_imgs]#batch size张图片的前一帧和后一帧\n",
    "        intrinsics = intrinsics.to(device)#(4,3,3)\n",
    "\n",
    "        # compute output\n",
    "        disparities = disp_net(tgt_img)# lenth 4 list of tensor(4,1,128,416) ,(4,1,64,208),(4,1,32,104),(4,1,16,52)]\n",
    "        depth = [1/disp for disp in disparities]\n",
    "        explainability_mask, pose = pose_exp_net(tgt_img, ref_imgs)\n",
    "        #pose (4,2,6)\n",
    "        #loss compute\n",
    "        loss_1, warped, diff = photometric_reconstruction_loss(tgt_img, ref_imgs, intrinsics,\n",
    "                                                               depth, explainability_mask, pose,\n",
    "                                                               args.rotation_mode, args.padding_mode)\n",
    "        if w2 > 0:\n",
    "            loss_2 = explainability_loss(explainability_mask)\n",
    "        else:\n",
    "            loss_2 = 0\n",
    "\n",
    "        loss_3 = smooth_loss(depth)\n",
    "\n",
    "        loss = w1*loss_1 + w2*loss_2 + w3*loss_3\n",
    "        \n",
    "        if log_losses:\n",
    "            train_writer.add_scalar('photometric_error', loss_1.item(), n_iter)\n",
    "            if w2 > 0:\n",
    "                train_writer.add_scalar('explanability_loss', loss_2.item(), n_iter)\n",
    "            train_writer.add_scalar('disparity_smoothness_loss', loss_3.item(), n_iter)\n",
    "            train_writer.add_scalar('total_loss', loss.item(), n_iter)\n",
    "\n",
    "        if log_output:\n",
    "            train_writer.add_image('train Input', tensor2array(tgt_img[0]), n_iter)\n",
    "            for k, scaled_maps in enumerate(zip(depth, disparities, warped, diff, explainability_mask)):\n",
    "                log_output_tensorboard(train_writer, \"train\", k, n_iter, *scaled_maps)\n",
    "\n",
    "        # record loss and EPE\n",
    "        losses.update(loss.item(), args.batch_size)\n",
    "\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        with open(args.save_path/args.log_full, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter='\\t')\n",
    "            writer.writerow([loss.item(), loss_1.item(), loss_2.item() if w2 > 0 else 0, loss_3.item()])\n",
    "        logger.train_bar.update(i+1)\n",
    "        if i % args.print_freq == 0:\n",
    "            logger.train_writer.write('Train: Time {} Data {} Loss {}'.format(batch_time, data_time, losses))\n",
    "        if i >= epoch_size - 1:\n",
    "            break\n",
    "\n",
    "        n_iter += 1\n",
    "\n",
    "    return losses.avg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_without_gt(args, val_loader, disp_net, pose_exp_net, epoch, logger, output_writers=[]):\n",
    "    global device\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter(i=3, precision=4)\n",
    "    log_outputs = len(output_writers) > 0\n",
    "    w1, w2, w3 = args.photo_loss_weight, args.mask_loss_weight, args.smooth_loss_weight\n",
    "    poses = np.zeros(((len(val_loader)-1) * args.batch_size * (args.sequence_length-1),6))\n",
    "    disp_values = np.zeros(((len(val_loader)-1) * args.batch_size * 3))\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    disp_net.eval()\n",
    "    pose_exp_net.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    logger.valid_bar.update(0)\n",
    "    for i, (tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(val_loader):\n",
    "        tgt_img = tgt_img.to(device)\n",
    "        ref_imgs = [img.to(device) for img in ref_imgs]\n",
    "        intrinsics = intrinsics.to(device)\n",
    "        intrinsics_inv = intrinsics_inv.to(device)\n",
    "\n",
    "        # compute output\n",
    "        disp = disp_net(tgt_img)\n",
    "        depth = 1/disp\n",
    "        explainability_mask, pose = pose_exp_net(tgt_img, ref_imgs)\n",
    "\n",
    "        loss_1, warped, diff = photometric_reconstruction_loss(tgt_img, ref_imgs,\n",
    "                                                               intrinsics, depth,\n",
    "                                                               explainability_mask, pose,\n",
    "                                                               args.rotation_mode, args.padding_mode)\n",
    "        loss_1 = loss_1.item()\n",
    "        if w2 > 0:\n",
    "            loss_2 = explainability_loss(explainability_mask).item()\n",
    "        else:\n",
    "            loss_2 = 0\n",
    "        loss_3 = smooth_loss(depth).item()\n",
    "\n",
    "        if log_outputs and i < len(output_writers):  # log first output of first batches\n",
    "            if epoch == 0:\n",
    "                for j,ref in enumerate(ref_imgs):\n",
    "                    output_writers[i].add_image('val Input {}'.format(j), tensor2array(tgt_img[0]), 0)\n",
    "                    output_writers[i].add_image('val Input {}'.format(j), tensor2array(ref[0]), 1)\n",
    "\n",
    "            log_output_tensorboard(output_writers[i], 'val', '', epoch, 1./disp, disp, warped, diff, explainability_mask)\n",
    "\n",
    "        if log_outputs and i < len(val_loader)-1:\n",
    "            step = args.batch_size*(args.sequence_length-1)\n",
    "            poses[i * step:(i+1) * step] = pose.cpu().view(-1,6).numpy()\n",
    "            step = args.batch_size * 3\n",
    "            disp_unraveled = disp.cpu().view(args.batch_size, -1)\n",
    "            disp_values[i * step:(i+1) * step] = torch.cat([disp_unraveled.min(-1)[0],\n",
    "                                                            disp_unraveled.median(-1)[0],\n",
    "                                                            disp_unraveled.max(-1)[0]]).numpy()\n",
    "\n",
    "        loss = w1*loss_1 + w2*loss_2 + w3*loss_3\n",
    "        losses.update([loss, loss_1, loss_2])\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        logger.valid_bar.update(i+1)\n",
    "        if i % args.print_freq == 0:\n",
    "            logger.valid_writer.write('valid: Time {} Loss {}'.format(batch_time, losses))\n",
    "    if log_outputs:\n",
    "        prefix = 'valid poses'\n",
    "        coeffs_names = ['tx', 'ty', 'tz']\n",
    "        if args.rotation_mode == 'euler':\n",
    "            coeffs_names.extend(['rx', 'ry', 'rz'])\n",
    "        elif args.rotation_mode == 'quat':\n",
    "            coeffs_names.extend(['qx', 'qy', 'qz'])\n",
    "        for i in range(poses.shape[1]):\n",
    "            output_writers[0].add_histogram('{} {}'.format(prefix, coeffs_names[i]), poses[:,i], epoch)\n",
    "        output_writers[0].add_histogram('disp_values', disp_values, epoch)\n",
    "    logger.valid_bar.update(len(val_loader))\n",
    "    return losses.avg, ['Total loss', 'Photo loss', 'Exp loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_with_gt(args, val_loader, disp_net, epoch, logger, output_writers=[]):\n",
    "    global device\n",
    "    batch_time = AverageMeter()\n",
    "    error_names = ['abs_diff', 'abs_rel', 'sq_rel', 'a1', 'a2', 'a3']\n",
    "    errors = AverageMeter(i=len(error_names))\n",
    "    log_outputs = len(output_writers) > 0\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    disp_net.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    logger.valid_bar.update(0)\n",
    "    for i, (tgt_img, depth) in enumerate(val_loader):\n",
    "        tgt_img = tgt_img.to(device)\n",
    "        depth = depth.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output_disp = disp_net(tgt_img)\n",
    "        output_depth = 1/output_disp[:,0]\n",
    "\n",
    "        if log_outputs and i < len(output_writers):\n",
    "            if epoch == 0:\n",
    "                output_writers[i].add_image('val Input', tensor2array(tgt_img[0]), 0)\n",
    "                depth_to_show = depth[0]\n",
    "                output_writers[i].add_image('val target Depth',\n",
    "                                            tensor2array(depth_to_show, max_value=10),\n",
    "                                            epoch)\n",
    "                depth_to_show[depth_to_show == 0] = 1000\n",
    "                disp_to_show = (1/depth_to_show).clamp(0,10)\n",
    "                output_writers[i].add_image('val target Disparity Normalized',\n",
    "                                            tensor2array(disp_to_show, max_value=None, colormap='magma'),\n",
    "                                            epoch)\n",
    "\n",
    "            output_writers[i].add_image('val Dispnet Output Normalized',\n",
    "                                        tensor2array(output_disp[0], max_value=None, colormap='magma'),\n",
    "                                        epoch)\n",
    "            output_writers[i].add_image('val Depth Output',\n",
    "                                        tensor2array(output_depth[0], max_value=10),\n",
    "                                        epoch)\n",
    "\n",
    "        errors.update(compute_errors(depth, output_depth))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        logger.valid_bar.update(i+1)\n",
    "        if i % args.print_freq == 0:\n",
    "            logger.valid_writer.write('valid: Time {} Abs Error {:.4f} ({:.4f})'.format(batch_time, errors.val[0], errors.avg[0]))\n",
    "    logger.valid_bar.update(len(val_loader))\n",
    "    return errors.avg, error_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
